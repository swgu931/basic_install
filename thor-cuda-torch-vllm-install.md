# howto do cuda, torch, vllm

# 1. í™•ì¸
```
- Thor ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ jetpackì—ì„œ ì§€ì›, ë³„ë„ cuda ì„¤ì¹˜ ë¶ˆí•„ìš”

which nvcc
nvcc --version
nvidia-smi
```


# howto check and install libcudart
```
ldconfig -p | grep libcudart  
ls -al /usr/local/cuda/lib64/libcudart.so

sudo apt update
sudo apt install -y libcudart12

ldconfig -p | grep libcudart

sudo apt-get install -y libnuma-dev

```

*ê°€ìƒí™˜ê²½ì—ì„œ ì‹œì‘í•  ê²ƒ (python 3.12ì´ì–´ì•¼ í•¨)
```
uv venv --python 3.12 vllm_env
source vllm_env/bin/activate
```


# howto install torch
```
pip uninstall -y torch torchvision torchaudio 
pip cache purge

python -m ensurepip --upgrade
python -m pip install --upgrade pip setuptools wheel

# ë²„ì „ í™•ì¸
python -m pip index versions torch --index-url https://download.pytorch.org/whl/cu129
python -m pip index versions torch --index-url https://download.pytorch.org/whl/cu130

python -m pip install torch==2.9.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu130


export TORCH_LIB="$(python - <<'PY'
import torch, os
print(os.path.join(os.path.dirname(torch.__file__), "lib"))
PY
)"
export LD_LIBRARY_PATH="$TORCH_LIB:$LD_LIBRARY_PATH"


# ë¡œë”ê°€ ì´ì œ ì°¾ëŠ”ì§€ í…ŒìŠ¤íŠ¸
python -c "import ctypes; ctypes.CDLL('libtorch_cuda.so'); print('libtorch_cuda.so load OK')"


# í˜¹ì€ ì•„ë˜ì™€ ê°™ì´ ì˜êµ¬ì ìš©

sudo tee /etc/ld.so.conf.d/torch.conf >/dev/null <<EOF
$TORCH_LIB
EOF
sudo ldconfig
ldconfig -p | grep libtorch_cuda || true

```



# howto check torch
```
1) ì„¤ì¹˜ëœ torchê°€ â€œCUDA ë¹Œë“œâ€ì´ê³  libtorch_cuda.soê°€ ìˆëŠ”ì§€ í™•ì¸
(1) torch ë¹Œë“œ í™•ì¸
python - <<'PY'
import torch
print("torch =", torch.__version__)
print("torch.version.cuda =", torch.version.cuda)
print("cuda available =", torch.cuda.is_available())
PY


torch.__version__ì— +cu130 ê°™ì€ í‘œê¸°ê°€ ë‚˜ì˜¤ê³ ,

torch.version.cudaê°€ 13.0(ë˜ëŠ” 13.x) ê³„ì—´ë¡œ ì°í˜€ì•¼ â€œCUDA ë¹Œë“œâ€ì…ë‹ˆë‹¤. (cu130 ì¸ë±ìŠ¤ ìì²´ëŠ” ì¡´ì¬í•©ë‹ˆë‹¤. )

(2) libtorch_cuda.so íŒŒì¼ ì¡´ì¬ í™•ì¸
python - <<'PY'
import torch, os
torch_lib = os.path.join(os.path.dirname(torch.__file__), "lib")
print("torch lib dir =", torch_lib)
PY

ls -al "$(python - <<'PY'
import torch, os
print(os.path.join(os.path.dirname(torch.__file__), "lib"))
PY
)" | grep -E "libtorch_cuda\.so|libtorch_cpu\.so" || true

ì•„ë˜ì™€ ê°™ì´ ë‚˜ì˜´
-rwxrwxr-x  1 lge lge 249319081 Jan 28 14:19 libtorch_cpu.so
-rwxrwxr-x  1 lge lge 509024049 Jan 28 14:19 libtorch_cuda.so

*ì—¬ê¸°ì„œ libtorch_cuda.soê°€ ì•„ì˜ˆ ì—†ìœ¼ë©´: ì§€ê¸ˆ ì„¤ì¹˜ëœ torchëŠ” CUDA íŒ¨í‚¤ì§€ì²˜ëŸ¼ ë³´ì´ë”ë¼ë„ ì‹¤ì œë¡œ CUDA libê°€ ë¹ ì§„ ë¹Œë“œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤(ê·¸ ê²½ìš° vLLMì€ ì ˆëŒ€ ëª» ëœ¹ë‹ˆë‹¤). â€œtorch/libì— libtorch_cuda.soê°€ ìˆì–´ì•¼ CUDA ì„¤ì¹˜ê°€ ë§ë‹¤â€ëŠ” í™•ì¸ë²•ë„ ë™ì¼í•˜ê²Œ ì•ˆë‚´ë©ë‹ˆë‹¤.

2) libtorch_cuda.soê°€ â€œìˆëŠ”ë°ë„â€ ëª» ì°¾ëŠ” ê²½ìš°: ë¼ì´ë¸ŒëŸ¬ë¦¬ ê²½ë¡œ ì¶”ê°€ (ê°€ì¥ í”í•¨)
(A) ì¦‰ì‹œ í…ŒìŠ¤íŠ¸(í˜„ì¬ í„°ë¯¸ë„ ì„¸ì…˜ë§Œ)
export TORCH_LIB="$(python - <<'PY'
import torch, os
print(os.path.join(os.path.dirname(torch.__file__), "lib"))
PY
)"
export LD_LIBRARY_PATH="$TORCH_LIB:$LD_LIBRARY_PATH"

# ë¡œë”ê°€ ì´ì œ ì°¾ëŠ”ì§€ í…ŒìŠ¤íŠ¸
python -c "import ctypes; ctypes.CDLL('libtorch_cuda.so'); print('libtorch_cuda.so load OK')"


(B) ì˜êµ¬ ì ìš©(ê¶Œì¥)

(ë§¤ë²ˆ export í•˜ê¸° ì‹«ìœ¼ë©´)

sudo tee /etc/ld.so.conf.d/torch.conf >/dev/null <<EOF
$TORCH_LIB
EOF
sudo ldconfig
ldconfig -p | grep libtorch_cuda || true
```


# howto setup in case of torch installed
```
export TORCH_LIB="$(python - <<'PY'
import torch, os
print(os.path.join(os.path.dirname(torch.__file__), "lib"))
PY
)"
export LD_LIBRARY_PATH="$TORCH_LIB:$LD_LIBRARY_PATH"


# ë¡œë”ê°€ ì´ì œ ì°¾ëŠ”ì§€ í…ŒìŠ¤íŠ¸
python -c "import ctypes; ctypes.CDLL('libtorch_cuda.so'); print('libtorch_cuda.so load OK')"


# í˜¹ì€ ì•„ë˜ì™€ ê°™ì´ ì˜êµ¬ì ìš©

sudo tee /etc/ld.so.conf.d/torch.conf >/dev/null <<EOF
$TORCH_LIB
EOF
sudo ldconfig
ldconfig -p | grep libtorch_cuda || true
```




# howto install vllm on nvidia thor (cuda, torch)
```
- https://docs.vllm.ai/en/latest/getting_started/installation/gpu/index.html#full-build

# install PyTorch first, either from PyPI or from source

git clone https://github.com/vllm-project/vllm.git
cd vllm


rm -rf build/ dist/ *.egg-info .eggs/ cmake-build-debug/ .deps/
python -m pip install -e . --no-build-isolation --no-cache-dir --no-deps


```



# #############################





# ###########################  torch &libcudart.12.so ì—†ëŠ” ë¬¸ì œ ë°œìƒ ì‹œ

```
ldconfig -p | grep libcudart  
ls -al /usr/local/cuda/lib64/libcudart.so

sudo apt update
sudo apt install -y libcudart12

ldconfig -p | grep libcudart


python -m pip uninstall -y torch torchvision torchaudio 
python -m pip cache purge

python -m ensurepip --upgrade
python -m pip install --upgrade pip setuptools wheel

# ë²„ì „ í™•ì¸
python -m pip index versions torch --index-url https://download.pytorch.org/whl/cu129
python -m pip index versions torch --index-url https://download.pytorch.org/whl/cu130
LD_LIBRARY_PATH
pip install torch==2.9.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu130

```




# ################################################################
```
python -m pip install "numpy==1.26.4" "scipy==1.11.4"

python -m pip uninstall -y opencv-python-headless opencv-python opencv-contrib-python opencv-contrib-python-headless


numpy, scipy version confict í•´ê²°


# í™•ì¸
which vllm
vllm --version
python -c "import vllm; print(vllm.__file__)"

```




rsync -avh --progress /tmp/EXAONE-4.0-32B-FP8 lge@10.231.182.159:~/ai-models/



# LD_LIBRARY_PATH ì„¤ì •
export TORCH_LIB="$(python - <<'PY'
import torch, os
print(os.path.join(os.path.dirname(torch.__file__), "lib"))
PY
)"
export LD_LIBRARY_PATH="$TORCH_LIB:$LD_LIBRARY_PATH"

### ë¡œë”ê°€ ì´ì œ ì°¾ëŠ”ì§€ í…ŒìŠ¤íŠ¸
python -c "import ctypes; ctypes.CDLL('libtorch_cuda.so'); print('libtorch_cuda.so load OK')"



# (ì¤‘ìš”) Hugging Face ë„¤íŠ¸ì›Œí¬ ì ‘ê·¼ ì™„ì „ ì°¨ë‹¨
export HF_HUB_OFFLINE=1
export TRANSFORMERS_OFFLINE=1
export HF_DATASETS_OFFLINE=1


# ptxas ê²½ë¡œ ì—†ì–´ì„œ ì—ëŸ¬
export CUDA_HOME=/usr/local/cuda
export PATH="${CUDA_HOME}/bin:${PATH}"
export TRITON_PTXAS_PATH="${CUDA_HOME}/bin/ptxas"

vllm serve /home/lge/ai-models/exaone-4.0-32b-fp8/ \
  --port 8080 \
  --max-model-len 32768 \VLLM_ATTENTION_BACKEND=FLASHINFER vllm serve /home/lge/ai-models/exaone-4.0-32b-fp8/ \
  --port 8080 \
  --max-model-len 32768 \
  --gpu-memory-utilization 0.85 \
  --enforce-eager
  --gpu-memory-utilization 0.85


# ì•„ë˜ ëª…ë ¹ì–´ë¡œ ì„±ê³µí•¨.
python -m vllm.entrypoints.openai.api_server \
  --model /home/lge/ai-models/exaone-4.0-32b-fp8/ \
  --port 8080 \
  --max-model-len 32768 \
  --gpu-memory-utilization 0.85


# ì§€ê¸ˆ ì—ëŸ¬ëŠ” â€œì»´íŒŒì¼ëœ PTXê°€ ë¬¸ì œâ€ë¼ì„œ, vLLM ì»´íŒŒì¼ ëª¨ë“œë¥¼ êº¼ì„œ PTX ìƒì„±/ë¡œë”©ì„ ìµœì†Œí™”í•˜ë©´ ìš°íšŒë˜ëŠ” ê²½ìš°ê°€ ìˆìŠµë‹ˆë‹¤.
vllm serve /home/lge/ai-models/exaone-4.0-32b-fp8/ \
  --port 8080 \
  --max-model-len 65536 \
  --gpu-memory-utilization 0.85 \
  --served-model-name LGAI-EXAONE/EXAONE-4.0-32B-FP8
  --enforce-eager



# 1. FlashInfer ì‚¬ìš© (Thorì—ì„œ ê¶Œì¥ë¨)
VLLM_ATTENTION_BACKEND=FLASHINFER vllm serve /home/lge/ai-models/exaone-4.0-32b-fp8/ \
  --port 8080 \
  --max-model-len 32768 \
  --gpu-memory-utilization 0.85 \
  --enforce-eager
# 2. ë§Œì•½ ìœ„ ë°©ë²•ì´ ì•ˆ ë˜ë©´ Xformers ì‚¬ìš©
# VLLM_ATTENTION_BACKEND=XFORMERS vllm serve ...  



## 
```
1. max-model-lenì˜ ì˜ë¯¸
max-model-lenì€ ëª¨ë¸ì´ í•œ ë²ˆì— ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ì…ë ¥(í”„ë¡¬í”„íŠ¸) + ì¶œë ¥(ìƒì„±) í† í°ì˜ ìµœëŒ€ ê¸¸ì´ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.

ì˜ˆì‹œ: 32768ë¡œ ì„¤ì •í•˜ë©´, í”„ë¡¬í”„íŠ¸ê°€ 30000 í† í°ì¼ ë•Œ ìµœëŒ€ 2768 í† í°ê¹Œì§€ë§Œ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ì´ ê°’ì„ ë„˜ì–´ì„œëŠ” ì…ë ¥ì´ ë“¤ì–´ì˜¤ë©´ ì—ëŸ¬ê°€ ë°œìƒí•˜ê±°ë‚˜ ë’·ë¶€ë¶„ì´ ì˜ë¦½ë‹ˆë‹¤.
ê°’ì„ í¬ê²Œ ì¡ì„ìˆ˜ë¡ ë©”ëª¨ë¦¬(KV Cache)ë¥¼ ë§ì´ ë¯¸ë¦¬ ì˜ˆì•½í•©ë‹ˆë‹¤.
2. EXAONE-4.0-32B-FP8 + 32k(32768) ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì˜ˆì¸¡
EXAONE 4.0 32BëŠ” GQA (Grouped Query Attention) ê¸°ìˆ ì„ ì‚¬ìš©í•˜ì—¬ ë©”ëª¨ë¦¬ íš¨ìœ¨ì´ ì¢‹ìŠµë‹ˆë‹¤ (Head 40ê°œ, KV Head 8ê°œ).

ì„¤ì •í•˜ì‹  32768 (32k) ê¸¸ì´ì— ë”°ë¥¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

ëª¨ë¸ ê°€ì¤‘ì¹˜ (ê³ ì •): ì•½ 34 GB (FP8 ëª¨ë¸ ê¸°ì¤€)
KV Cache (context ê¸¸ì´ì— ë¹„ë¡€):
FP16 Cache ì‚¬ìš© ì‹œ: ì•½ 8.6 GB
FP8 Cache ì‚¬ìš© ì‹œ: ì•½ 4.3 GB (vLLM ê¸°ë³¸ê°’, ë˜ëŠ” --kv-cache-dtype fp8 ì˜µì…˜)
ê¸°íƒ€ ì˜¤ë²„í—¤ë“œ (CUDA Context ë“±): ì•½ 1~2 GB
ğŸ”¹ ì´ ì˜ˆìƒ ì†Œìš” ë©”ëª¨ë¦¬ (VRAM)
ì•½ 40GB ~ 44GB
ë§Œì•½ NVIDIA Thorì˜ ê°€ìš© ë©”ëª¨ë¦¬ê°€ ì´ë¥¼ ì¶©ì¡±í•œë‹¤ë©´ ì•ˆì •ì ìœ¼ë¡œ ì‹¤í–‰ë  ê²ƒì…ë‹ˆë‹¤. ë§Œì•½ ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•˜ë‹¤ë©´, max-model-lenì„ 8192 (ì•½ 1GB Cache) ë‚˜ 16384 (ì•½ 2GB Cache) ì •ë„ë¡œ ì¤„ì´ê±°ë‚˜, --gpu-memory-utilization ì˜µì…˜ì„ ì¡°ì ˆí•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ ê³µê°„ì„ ì¤„ì—¬ì•¼ í•©ë‹ˆë‹¤.




*32768(32k)ë³´ë‹¤ ë” ëŠ˜ë¦¬ì‹ ë‹¤ë©´, **2ë°° ë‹¨ìœ„(Powers of 2)**ë¡œ ëŠ˜ë¦¬ëŠ” ê²ƒì´ ê°€ì¥ íš¨ìœ¨ì ì´ê³  ì¼ë°˜ì ì…ë‹ˆë‹¤.

ì¶”ì²œ ì„¤ì •: 65536 (64k) ë˜ëŠ” 131072 (128k)
1. ì™œ 2ë°° ë‹¨ìœ„ì¸ê°€ìš”?
ë©”ëª¨ë¦¬ ê´€ë¦¬ íš¨ìœ¨ì„±: ì»´í“¨í„° ì‹œìŠ¤í…œ(íŠ¹íˆ GPU ë° vLLMì˜ PagedAttention)ì€ ë©”ëª¨ë¦¬ ë¸”ë¡ì„ í• ë‹¹í•  ë•Œ 2ì˜ ê±°ë“­ì œê³± ë‹¨ìœ„ë¡œ ê´€ë¦¬í•˜ëŠ” ê²ƒì´ ì˜¤ë²„í—¤ë“œê°€ ê°€ì¥ ì ìŠµë‹ˆë‹¤.
RoPE (ìœ„ì¹˜ ì¸ì½”ë”©): LLMì˜ ìœ„ì¹˜ ì •ë³´ ì²˜ë¦¬(Rotary Positional Embedding) ì•Œê³ ë¦¬ì¦˜ë“¤ì´ ë³´í†µ 2k, 4k, 8k, 16k, 32k ë‹¨ìœ„ë¡œ í•™ìŠµë˜ê±°ë‚˜ í…ŒìŠ¤íŠ¸ë©ë‹ˆë‹¤. ëœ¬ê¸ˆì—†ëŠ” ìˆ«ì(ì˜ˆ: 40000, 50000)ë³´ë‹¤ëŠ” 65536(64k)ì´ ìˆ˜ì¹˜ì  ì•ˆì •ì„±ì´ ë” ë†’ìŠµë‹ˆë‹¤.
2. ë©”ëª¨ë¦¬ ì¦ê°€ëŸ‰ ì˜ˆì¸¡ (EXAONE-32B F8 Cache ê¸°ì¤€)
ê¸¸ì´ë¥¼ ëŠ˜ë¦¬ë©´ **KV Cache ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì€ ê¸¸ì´ì— ì •ë¹„ë¡€(Linear)**í•´ì„œ ëŠ˜ì–´ë‚©ë‹ˆë‹¤.

32,768 (32k): ~4.3 GB (í˜„ì¬)
65,536 (64k): ~8.6 GB (ì¶”ì²œ)
131,072 (128k): ~17.2 GB
3. ê²°ë¡  ë° ì£¼ì˜ì‚¬í•­
EXAONE 4.0 ëª¨ë¸ì€ ìµœëŒ€ 128kê¹Œì§€ ì§€ì›í•˜ë¯€ë¡œ, Thor ì¥ë¹„ì˜ ë©”ëª¨ë¦¬ì— ì—¬ìœ ê°€ ìˆë‹¤ë©´ 65536ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ í…ŒìŠ¤íŠ¸í•´ë³´ì‹œëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.



* --enforce-eager ì˜µì…˜ì€ vLLMì´ CUDA Graph ìµœì í™”ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³ , PyTorchì˜ Eager Execution ëª¨ë“œ(ì¦‰ì‹œ ì‹¤í–‰ ëª¨ë“œ)ë¡œ ë™ì‘í•˜ë„ë¡ ê°•ì œí•˜ëŠ” ì˜µì…˜ì…ë‹ˆë‹¤.

1. ì—­í• 
CUDA Graph ë¹„í™œì„±í™”: vLLMì€ ê¸°ë³¸ì ìœ¼ë¡œ ì„±ëŠ¥ì„ ìœ„í•´ CUDA Graph(ì—°ì‚° ê·¸ë˜í”„ë¥¼ ë¯¸ë¦¬ ìº¡ì²˜í•´ì„œ ì¬ì‚¬ìš©í•˜ëŠ” ê¸°ìˆ )ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ ì˜µì…˜ì€ ê·¸ ê¸°ëŠ¥ì„ ë•ë‹ˆë‹¤.
ì¦‰ì‹œ ì‹¤í–‰: ì—°ì‚°ì„ ë¯¸ë¦¬ ì»´íŒŒì¼í•˜ê±°ë‚˜ ìº¡ì²˜í•˜ì§€ ì•Šê³ , íŒŒì´ì¬ ì½”ë“œê°€ ì‹¤í–‰ë˜ëŠ” ìˆœì„œëŒ€ë¡œ GPUì— ì¦‰ì‹œ ëª…ë ¹ì„ ë³´ëƒ…ë‹ˆë‹¤.
2. ì–¸ì œ/ì™œ ì‚¬ìš©í•˜ë‚˜?
í˜¸í™˜ì„± ë¬¸ì œ í•´ê²° (ì£¼ëœ ì´ìœ ): ì¼ë¶€ GPU(íŠ¹íˆ Thor ê°™ì€ ìµœì‹ /ì„ë² ë””ë“œ SoCë‚˜ êµ¬í˜• GPU)ë‚˜ íŠ¹ì • ëª¨ë¸ ì•„í‚¤í…ì²˜ì—ì„œ CUDA Graph ìº¡ì²˜ê°€ ì‹¤íŒ¨í•˜ê±°ë‚˜ ë²„ê·¸ê°€ ìˆì„ ë•Œ, ì´ ì˜µì…˜ì„ ì¼œë©´ ì†ë„ëŠ” ì¡°ê¸ˆ ëŠë ¤ì ¸ë„ ì•ˆì •ì ìœ¼ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.
ë””ë²„ê¹…: ì—ëŸ¬ê°€ ì–´ë””ì„œ ë‚˜ëŠ”ì§€ ì •í™•í•œ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ë¥¼ ë³´ê³  ì‹¶ì„ ë•Œ ìœ ìš©í•©ë‹ˆë‹¤.
ë©”ëª¨ë¦¬ ì ˆì•½: CUDA GraphëŠ” ê·¸ë˜í”„ ìº¡ì²˜ë¥¼ ìœ„í•´ ì¶”ê°€ì ì¸ GPU ë©”ëª¨ë¦¬ë¥¼ í•„ìš”ë¡œ í•©ë‹ˆë‹¤. ë©”ëª¨ë¦¬ê°€ ì •ë§ ë¶€ì¡±í•  ë•Œ ì´ ì˜µì…˜ì„ ì¼œë©´ ì•½ê°„ì˜ ë©”ëª¨ë¦¬ë¥¼ ì•„ë‚„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
3. ë‹¨ì 
ì„±ëŠ¥ ì €í•˜: CUDA Graphë¥¼ ì“°ì§€ ì•Šìœ¼ë©´ ì‘ì€ ë°°ì¹˜ë¥¼ ì²˜ë¦¬í•  ë•Œ ì˜¤ë²„í—¤ë“œê°€ ì»¤ì ¸ì„œ **í† í° ìƒì„± ì†ë„(TPS)**ê°€ ì•½ 10~30% ì •ë„ ëŠë ¤ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ìš”ì•½í•˜ìë©´: ì§€ê¸ˆ Thor í™˜ê²½ì—ì„œ CUDA Graph ê´€ë ¨ í˜¸í™˜ì„± ë¬¸ì œë‚˜ ì—ëŸ¬ê°€ ë°œìƒí–ˆì„ ê°€ëŠ¥ì„±ì´ ìˆì–´, ì•ˆì „í•˜ê²Œ ì‹¤í–‰í•˜ê¸° ìœ„í•´ í•´ë‹¹ ì˜µì…˜ì„ ì¼œë‘” ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤. (ë§Œì•½ ì—ëŸ¬ ì—†ì´ ì˜ ëŒì•„ê°„ë‹¤ë©´ ë„ëŠ” ê²ƒì´ ì„±ëŠ¥ìƒ ìœ ë¦¬í•©ë‹ˆë‹¤.)

```




# ì—ëŸ¬ : (EngineCore_DP0 pid=31913) ERROR 01-28 18:31:43 [core.py:946] torch.AcceleratorError: CUDA error: the provided PTX was compiled with an unsupported toolchain.
```
nvidia-smi
nvcc --version || true
/usr/local/cuda/bin/ptxas --version || true

 /usr/local/cuda/bin/ptxas --version || true
ptxas: NVIDIA (R) Ptx optimizing assembler
Copyright (c) 2005-2025 NVIDIA Corporation
Built on Tue_Dec_16_07:22:57_PM_PST_2025
Cuda compilation tools, release 13.1, V13.1.115
Build cuda_13.1.r13.1/compiler.37061995_0

export CUDA_HOME=/usr/local/cuda
export PATH="$CUDA_HOME/bin:$PATH"
export LD_LIBRARY_PATH="$CUDA_HOME/lib64:$LD_LIBRARY_PATH"
export TRITON_PTXAS_PATH="$CUDA_HOME/bin/ptxas"


rm -rf ~/.cache/torch/inductor ~/.triton ~/.cache/triton
```




# To test service

```
curl http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_API_KEY" \  # ì—†ìœ¼ë©´ ìƒëµ ê°€ëŠ¥
  -d '{
    "model": "LGAI-EXAONE/EXAONE-4.0-32B-FP8",
    "messages": [{"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”, í•œêµ­ì–´ë¡œ ëŒ€í™”í•  ìˆ˜ ìˆë‚˜ìš”?"}],
    "temperature": 0.1
  }'
  
   YOUR_API_KEYëŠ” ê¸°ë³¸ê°’ìœ¼ë¡œ secretì´ë©°, --api-key ì˜µì…˜ìœ¼ë¡œ ë³€ê²½ ê°€ëŠ¥


curl http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "LGAI-EXAONE/EXAONE-4.0-32B-FP8",
    "messages": [{"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”, í•œêµ­ì–´ë¡œ ëŒ€í™”í•  ìˆ˜ ìˆë‚˜ìš”?"}],
    "temperature": 0.1
  }'

curl http://10.231.182.159:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "LGAI-EXAONE/EXAONE-4.0-32B-FP8",
    "messages": [{"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”, í•œêµ­ì–´ë¡œ ëŒ€í™”í•  ìˆ˜ ìˆë‚˜ìš”?"}],
    "temperature": 0.1
  }'

   YOUR_API_KEYëŠ” ê¸°ë³¸ê°’ìœ¼ë¡œ secretì´ë©°, --api-key ì˜µì…˜ìœ¼ë¡œ ë³€ê²½ ê°€ëŠ¥   
```   


# To change option
```
5.1. ì–‘ìí™”(Quantization) ì„¤ì •
FP8, AWQ, GPTQ ë“± ë‹¤ì–‘í•œ ì–‘ìí™” ë°©ì‹ì„ ì§€ì›í•©ë‹ˆë‹¤.

BASH

vllm serve LGAI-EXAONE/EXAONE-4.0-32B-FP8 \
  --quantization fp8 \
  --dtype fp8
5.2. GPU ë©”ëª¨ë¦¬ ìµœì í™”
BASH

vllm serve LGAI-EXAONE/EXAONE-4.0-32B-FP8 \
  --gpu-memory-fraction 0.9 \  # GPU ë©”ëª¨ë¦¬ì˜ 90% ì‚¬ìš©
  --max-num-batched-tokens 4096
5.3. API í‚¤ ì„¤ì •
BASH

vllm serve LGAI-EXAONE/EXAONE-4.0-32B-FP8 \
  --api-key my-secret-key
```   
